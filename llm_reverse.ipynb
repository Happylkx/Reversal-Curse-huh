{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaixin/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from peft import PeftModel\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "\n",
    "def load_model(model_name, lora_path=None):\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer_name = model_name\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        use_fast=False,  # Take care of llama\n",
    "        trust_remote_code=True,\n",
    "        padding_side='left'\n",
    "    )\n",
    "    # Workaround for LLaMA tokenizers\n",
    "    if \"llama\" in model_name.lower():\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    if lora_path:\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_path,\n",
    "            torch_dtype=torch.bfloat16,\n",
    "        )\n",
    "        print(f\"Loaded peft model from {lora_path}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tags1.1=1'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([917, 29896, 29889, 29896, 29922, 29896])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(text: str, model, tokenizer):\n",
    "    pattern = re.compile(r'(<(\\w+)>(.+?)</\\2>)|([^<>]+)')\n",
    "    \n",
    "    cumulative_text = \"\"\n",
    "    span_names = []\n",
    "    span_starts = []  # span start token positions\n",
    "    span_ends = []  # span end token positions\n",
    "    span_contents = []\n",
    "\n",
    "    for match in pattern.finditer(text):\n",
    "        text_before_span = cumulative_text\n",
    "        \n",
    "        if match.group(1):\n",
    "            span_names.append(match.group(2))\n",
    "            span_text = match.group(3)\n",
    "            span_contents.append(span_text)\n",
    "        else:\n",
    "            span_text = match.group(0)\n",
    "        \n",
    "        cumulative_text += span_text\n",
    "\n",
    "        if match.group(1):\n",
    "            span_starts.append(len(tokenizer.tokenize(text_before_span)))\n",
    "            span_ends.append(len(tokenizer.tokenize(cumulative_text)))\n",
    "    \n",
    "    # ASSERT THAT THE CUMULATIVE TEXT IS EQUAL TO THE INPUT TEXT WITH OUT TAGS\n",
    "    assert cumulative_text == re.sub(r'<.+?>|</.+?>', \"\", text)\n",
    "\n",
    "    # Forward the model to find the unconditional probabilities of the spans\n",
    "    uncond_span_probs = {}\n",
    "    for span_name, span_content in zip(span_names, span_contents):\n",
    "        uncond_span_input = tokenizer(span_content, return_tensors=\"pt\").to(model.device)\n",
    "        uncond_logits = model(**uncond_span_input, return_dict=True).logits\n",
    "        uncond_probs = torch.softmax(uncond_logits, 2)\n",
    "        target_span_probs = torch.gather(uncond_probs[0, :-1], -1, uncond_span_input.input_ids[0, 1:].unsqueeze(-1))\n",
    "        uncond_span_probs[span_name] = target_span_probs.mean().item()\n",
    "\n",
    "\n",
    "    inputs = tokenizer(cumulative_text, return_tensors=\"pt\").to(model.device)\n",
    "    # Forward the model to find the conditional probabilities of the spans\n",
    "    outputs = model(**inputs, return_dict=True)\n",
    "    logits = outputs.logits\n",
    "    probs = torch.softmax(logits, 2)\n",
    "    seq_probs = probs[0]\n",
    "    # Extract scores for each span.\n",
    "    cond_span_probs = {}\n",
    "    for span_name, start, end in zip(span_names, span_starts, span_ends):\n",
    "        # print(start, end)\n",
    "        # print(seq_probs[start-1:end-1])\n",
    "        target_probs = torch.gather(seq_probs[start-1:end-1], -1, inputs.input_ids[0, start:end].unsqueeze(-1))\n",
    "        cond_span_probs[span_name] = target_probs.mean().item()\n",
    "    \n",
    "    print(\"Unconditional probabilities:\")\n",
    "    for span_name in span_names:\n",
    "        print(f\"p({span_name}) = {uncond_span_probs[span_name]}\")\n",
    "\n",
    "    print(\"\\nConditional probabilities:\")\n",
    "    history_span_names = []\n",
    "    for span_name in span_names:\n",
    "        if not history_span_names:\n",
    "            print(f\"p({span_name} | <prompt_prefix>) = {cond_span_probs[span_name]}\")\n",
    "        else:\n",
    "            print(f\"p({span_name} | {', '.join(history_span_names)}) = {cond_span_probs[span_name]}\")\n",
    "        history_span_names.append(span_name)\n",
    "\n",
    "\n",
    "\n",
    "    return uncond_span_probs, cond_span_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unconditional probabilities:\n",
      "p(A) = 0.00778670608997345\n",
      "p(B) = 0.007787714712321758\n",
      "\n",
      "Conditional probabilities:\n",
      "p(A | <prompt_prefix>) = 0.0022087732795625925\n",
      "p(B | A) = 0.30144742131233215\n"
     ]
    }
   ],
   "source": [
    "probs = get_probs(\"\"\"An<A> apple</A> is an<B> English</B> word.\"\"\", model, tokenizer)  # An / A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 530, 26163, 338, 263, 15774, 29889], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"An apple is a fruit.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
